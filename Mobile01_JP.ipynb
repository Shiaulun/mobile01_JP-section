{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re,requests,math,time\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set type of address\n",
    "\n",
    "url0 ='https://www.mobile01.com/' #mobile01 homepage\n",
    "url1 ='waypointtopiclist.php?f=405&p='  \n",
    "url2 = url0 + 'waypointtopicdetail.php?f=405&'  #mobile01 travel homepage\n",
    "first_page_url = url0 + url1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.mobile01.com/waypointtopiclist.php?f=405&p='"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_page_url  # preview first_page_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# definite a function for getting all pages link\n",
    "\n",
    "def get_all_pages_link(first_page_url):\n",
    "    links = []\n",
    "    for i in range(1,int(max_page)+1):\n",
    "        links.append(first_page_url + str(i))\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the max_page of this section\n",
    "res = requests.get(first_page_url)\n",
    "soup = BeautifulSoup(res.text,'lxml')\n",
    "max_page = soup.select('p.numbers')[0].text.split('共')[1].split('頁')[0]  \n",
    "\n",
    "# definite a function for getting all topic links\n",
    "def get_all_topic_links(first_page_url):\n",
    "    links = []\n",
    "    page = 1\n",
    "    for url in get_all_pages_link(first_page_url):\n",
    "        res = requests.get(url)\n",
    "        soup = BeautifulSoup(res.text,'lxml')\n",
    "        \n",
    "        # because article link contain top and gen(stick-top topic and normal topic) \n",
    "        # use try and except to avoid error(link is not exist)\n",
    "        try:\n",
    "            top_links = []\n",
    "            for i in [i for i in soup.select('span > a.topic_top')]:\n",
    "                top_links.append('https://www.mobile01.com/waypointtop' + i['href'].split('top')[1])\n",
    "            gen_links = []\n",
    "            for i in [i for i in soup.select('span > a.topic_gen')]:\n",
    "                gen_links.append('https://www.mobile01.com/waypointtop' + i['href'].split('top')[1])\n",
    "            print('page{} is OK!'.format(page))\n",
    "            links += top_links + gen_links\n",
    "        except:\n",
    "            gen_links = []\n",
    "            for i in [i for i in soup.select('span > a.topic_gen')]:\n",
    "                gen_links.append('https://www.mobile01.com/waypointtop' + i['href'].split('top')[1])\n",
    "            print('page{} is OK!'.format(page))\n",
    "            links +=gen_links\n",
    "        time.sleep(1)\n",
    "        page+=1\n",
    "    return links\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_topic_links = get_all_topic_links(first_page_url)  #preview all topic links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### save all topic links to avoid to continue post request to the server  ###\n",
    "\n",
    "# for i in all_page_links:\n",
    "#     f = open(\"C:\\\\Users\\\\Java\\\\Desktop\\\\links.text\", \"a\",encoding='utf8')   #workspace\n",
    "#     f.write(i+'\\n')\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### read all topic links from workspace ###\n",
    "\n",
    "# f = open(\"C:\\\\Users\\\\Java\\\\Desktop\\\\links.text\", \"r\",encoding='utf8')  #workspace\n",
    "# all_link = []\n",
    "# for i in f:\n",
    "#     all_link.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7702"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### preview all link ###\n",
    "\n",
    "# len(all_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inside_page_link = url0 + soup.select('span > a.topic_gen')[0]['href'] + '&p='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# url = inside_page_link\n",
    "# res = requests.get(url)\n",
    "# soup = BeautifulSoup(res.text,'lxml')\n",
    "# max_inside_page = int(soup.select('.contentfoot')[0].text.split('共')[1].split('頁')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### start this function to get all topic and save to the workspace ##\n",
    "\n",
    "start_topic =1\n",
    "test = {}\n",
    "for link in all_topic_links:\n",
    "    url = link + '&p='\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text,'lxml')\n",
    "    max_inside_page = int(soup.select('.contentfoot')[0].text.split('共')[1].split('頁')[0])\n",
    "    \n",
    "    author_info = soup.select('div > .single-post-author')[0].text.strip().replace('\\n\\n','\\n').replace('私訊連結','').split('\\n')\n",
    "#     try:\n",
    "    for s in author_info:\n",
    "        if s == '無圖示': \n",
    "            author_info.remove(s)\n",
    "    test['作者'] = author_info[0]\n",
    "    test['標題'] = soup.select('.topic')[0].text \n",
    "    test['發文時間'] = author_info[2].split(' ')[0]\n",
    "    test['人氣'] = author_info[1].split(' ')[1].replace(',','')\n",
    "    test['來源'] = \"mobile01\"\n",
    "    \n",
    "    test['文章內容'] = soup.select('div.single-post')[0].text.split('\\n\\n\\n\\n')[1]\n",
    "    test['url'] = link\n",
    "     \n",
    "    #     \n",
    "\n",
    "    # #     \n",
    "\n",
    "    total_author =soup.select('div > .fn')\n",
    "    \n",
    "    cm1 =[]\n",
    "    for i in range(1,len(total_author)):\n",
    "        author_info = soup.select('div > .single-post-author')[i].text.strip().replace('\\n\\n','\\n').replace('私訊連結','').split('\\n')\n",
    "\n",
    "        \n",
    "        for s in author_info:\n",
    "            if s == '無圖示': \n",
    "                author_info.remove(s)\n",
    "\n",
    "        author = author_info[0]\n",
    "        article_date = author_info[2].split(' ')[0]\n",
    "        content = soup.select('div.single-post')[i].text.split('\\n\\n\\n\\n')[1]\n",
    "        dd ={'作者':author,'發文日期':article_date,'回文內容':content}\n",
    "        cm1.append(dd)\n",
    "    \n",
    "    time.sleep(randint(3,5))\n",
    "    \n",
    "\n",
    "    t2_author = []\n",
    "    t2_date = []\n",
    "    t2_content = []\n",
    "    cm2 = []\n",
    "    for i in range(2,max_inside_page+1):\n",
    "        inside_page_url = url + str(i)\n",
    "        res = requests.get(inside_page_url)\n",
    "        soup = BeautifulSoup(res.text,'lxml')\n",
    "        total_author =soup.select('div > .fn')\n",
    "        \n",
    "          \n",
    "        \n",
    "        for j in range(0,len(total_author)):\n",
    "            author_info = soup.select('div > .single-post-author')[j].text.strip().replace('\\n\\n','\\n').replace('私訊連結','').split('\\n')\n",
    "            for s in author_info:\n",
    "                if s == '無圖示': \n",
    "                    author_info.remove(s)\n",
    "            \n",
    "            author = author_info[0]\n",
    "            article_date = author_info[2].split(' ')[0]\n",
    "            content = soup.select('div.single-post')[j].text.split('\\n\\n\\n\\n')[1].split('\\n\\n\\n')[0]\n",
    "#             try:\n",
    "#                 if re.search('(恕刪)',content):\n",
    "#                     content = soup.select('div.single-post')[j].text.split('\\n\\n\\n\\n')[1].split('(恕刪)')[1]\n",
    "#                 else:\n",
    "#                     content = soup.select('div.single-post')[j].text.split('\\n\\n\\n\\n')[1]\n",
    "#             except:\n",
    "#                 pass\n",
    "            dd ={'作者':author,'發文日期':article_date,'回文內容':content}\n",
    "            \n",
    "            cm2.append(dd)\n",
    "  \n",
    "        time.sleep(randint(3,5))\n",
    "    cm = cm1 + cm2\n",
    "    \n",
    "    test['回文'] = cm\n",
    "    \n",
    "      \n",
    "\n",
    " \n",
    "\n",
    "    \n",
    "    dest_path = \"C:\\\\Users\\\\Java\\\\Desktop\\\\mobile01_JP\\\\command{}.text\".format(start_topic)\n",
    "    with open(dest_path,'a',encoding=\"utf-8\") as f:\n",
    "#         f.write(str(test))\n",
    "        f.write(json.dumps(test,sort_keys=True,ensure_ascii=False))\n",
    "        f.close()\n",
    "    \n",
    "    print('第{}則文章以爬取完成'.format(start_topic))\n",
    "    time.sleep(randint(1,3)) \n",
    "    start_topic += 1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### use os method to get the file from workspace directory ##\n",
    "import  os\n",
    "DATA_DIR = \"C:\\\\Users\\\\Java\\\\Desktop\\\\mobile01_JP\\\\\"\n",
    "file_data = []\n",
    "for filename in os.listdir(DATA_DIR):\n",
    "    print( \"Loading: {}\".format(filename))\n",
    "    loadFile = open(os.path.join(DATA_DIR, filename), 'r', encoding='UTF-8')\n",
    "    file_data.append(loadFile.read())\n",
    "    loadFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('10.120.27.11', 27017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in file_data:\n",
    "    i = json.loads(i)\n",
    "    \n",
    "    client.text_mining.mobile01_article.insert_one(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
